{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 序列逆置 （加注意力的seq2seq）\n",
    "使用attentive sequence to sequence 模型将一个字符串序列逆置。例如 `OIMESIQFIQ` 逆置成 `QIFQISEMIO`(下图来自网络，是一个加attentino的sequence to sequence 模型示意图)\n",
    "![attentive seq2seq](./seq2seq-attn.jpg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T06:28:07.242032200Z",
     "start_time": "2024-12-21T06:28:03.392801500Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "import os,sys,tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 玩具序列数据生成\n",
    "生成只包含[A-Z]的字符串，并且将encoder输入以及decoder输入以及decoder输出准备好（转成index）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T06:28:07.265480900Z",
     "start_time": "2024-12-21T06:28:07.242032200Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(['IVYYVQLBFL', 'JQIYNSYWUV'], <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
      "array([[ 9, 22, 25, 25, 22, 17, 12,  2,  6, 12],\n",
      "       [10, 17,  9, 25, 14, 19, 25, 23, 21, 22]])>, <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
      "array([[ 0, 12,  6,  2, 12, 17, 22, 25, 25, 22],\n",
      "       [ 0, 22, 21, 23, 25, 19, 14, 25,  9, 17]])>, <tf.Tensor: shape=(2, 10), dtype=int32, numpy=\n",
      "array([[12,  6,  2, 12, 17, 22, 25, 25, 22,  9],\n",
      "       [22, 21, 23, 25, 19, 14, 25,  9, 17, 10]])>)\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import string\n",
    "\n",
    "def randomString(stringLength):\n",
    "    \"\"\"Generate a random string with the combination of lowercase and uppercase letters \"\"\"\n",
    "\n",
    "    letters = string.ascii_uppercase\n",
    "    return ''.join(random.choice(letters) for i in range(stringLength))\n",
    "\n",
    "def get_batch(batch_size, length):\n",
    "    batched_examples = [randomString(length) for i in range(batch_size)]\n",
    "    enc_x = [[ord(ch)-ord('A')+1 for ch in list(exp)] for exp in batched_examples]\n",
    "    y = [[o for o in reversed(e_idx)] for e_idx in enc_x]\n",
    "    dec_x = [[0]+e_idx[:-1] for e_idx in y]\n",
    "    return (batched_examples, tf.constant(enc_x, dtype=tf.int32), \n",
    "            tf.constant(dec_x, dtype=tf.int32), tf.constant(y, dtype=tf.int32))\n",
    "print(get_batch(2, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 建立sequence to sequence 模型\n",
    "\n",
    "完成两空，模型搭建以及单步解码逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T06:28:07.286150100Z",
     "start_time": "2024-12-21T06:28:07.261756300Z"
    }
   },
   "outputs": [],
   "source": [
    "class mySeq2SeqModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super(mySeq2SeqModel, self).__init__()\n",
    "        self.v_sz=27\n",
    "        self.hidden = 128\n",
    "        self.embed_layer = tf.keras.layers.Embedding(self.v_sz, 64, \n",
    "                                                    batch_input_shape=[None, None])\n",
    "        \n",
    "        self.encoder_cell = tf.keras.layers.SimpleRNNCell(self.hidden)\n",
    "        self.decoder_cell = tf.keras.layers.SimpleRNNCell(self.hidden)\n",
    "        \n",
    "        self.encoder = tf.keras.layers.RNN(self.encoder_cell, \n",
    "                                           return_sequences=True, return_state=True)\n",
    "        self.decoder = tf.keras.layers.RNN(self.decoder_cell, \n",
    "                                           return_sequences=True, return_state=True)\n",
    "        self.dense_attn = tf.keras.layers.Dense(self.hidden)\n",
    "        self.dense = tf.keras.layers.Dense(self.v_sz)\n",
    "        \n",
    "        \n",
    "    # @tf.function\n",
    "    # def call(self, enc_ids, dec_ids):\n",
    "    #     '''\n",
    "    #     todo\n",
    "    #     \n",
    "    #     完成带attention机制的 sequence2sequence 模型的搭建，模块已经在`__init__`函数中定义好，\n",
    "    #     用双线性attention，或者自己改一下`__init__`函数做加性attention\n",
    "    #     '''\n",
    "    #     \n",
    "    #     \n",
    "    #     return logits\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, enc_ids, dec_ids):\n",
    "        \"\"\"Sequence-to-sequence with attention mechanism\"\"\"\n",
    "        \n",
    "        # 编码器\n",
    "        enc_emb = self.embed_layer(enc_ids)  # (batch_size, seq_len, embed_size)\n",
    "        enc_out, enc_state = self.encoder(enc_emb)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # 解码器输入\n",
    "        dec_emb = self.embed_layer(dec_ids)  # (batch_size, seq_len, embed_size)\n",
    "        dec_out, dec_state = self.decoder(dec_emb, initial_state=enc_state)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # 注意力机制\n",
    "        # 计算解码器输出与编码器输出之间的注意力权重\n",
    "        # 使用加性注意力：查询和键的组合通过一个神经网络来计算相似度\n",
    "        attn_scores = self.dense_attn(dec_out)  # (batch_size, seq_len, hidden_size)\n",
    "        attn_scores = tf.matmul(attn_scores, enc_out, transpose_b=True)  # (batch_size, seq_len, seq_len)\n",
    "        \n",
    "        # 计算注意力权重\n",
    "        attn_weights = tf.nn.softmax(attn_scores, axis=-1)  # (batch_size, seq_len, seq_len)\n",
    "\n",
    "        # 计算上下文向量（context vector），它是编码器输出的加权和\n",
    "        context_vector = tf.matmul(attn_weights, enc_out)  # (batch_size, seq_len, hidden_size)\n",
    "\n",
    "        # 结合上下文向量和解码器的输出\n",
    "        dec_out_with_ctx = tf.concat([dec_out, context_vector], axis=-1)  # (batch_size, seq_len, 2 * hidden_size)\n",
    "\n",
    "        # 生成最终预测 logits\n",
    "        logits = self.dense(dec_out_with_ctx)  # (batch_size, seq_len, vocab_size)\n",
    "        return logits\n",
    "    \n",
    "    \n",
    "    # @tf.function\n",
    "    def encode(self, enc_ids):\n",
    "        enc_emb = self.embed_layer(enc_ids) # shape(b_sz, len, emb_sz)\n",
    "        enc_out, enc_state = self.encoder(enc_emb)\n",
    "        return enc_out, [enc_out[:, -1, :], enc_state]\n",
    "        # return enc_out, enc_state\n",
    "    \n",
    "    \n",
    "    # def get_next_token(self, x, state, enc_out):\n",
    "    #     '''\n",
    "    #     shape(x) = [b_sz,] \n",
    "    #     '''\n",
    "    # \n",
    "    #     '''\n",
    "    #     todo\n",
    "    #     参考sequence_reversal-exercise, 自己构建单步解码逻辑'''\n",
    "    # \n",
    "    #     return out, state\n",
    "    \n",
    "    def get_next_token(self, x, state, enc_out):\n",
    "        '''\n",
    "        shape(x) = [b_sz,] \n",
    "        '''\n",
    "\n",
    "        '''\n",
    "        todo\n",
    "        参考sequence_reversal-exercise, 自己构建单步解码逻辑'''\n",
    "\n",
    "        dec_emb = self.embed_layer(x) # shape(b_sz, emb_sz)\n",
    "        h, state = self.decoder_cell.call(dec_emb, state) # shape(b_sz, hidden_sz)\n",
    "        attn_scores = self.dense_attn(h)  # shape(b_sz, hidden_sz)\n",
    "        \n",
    "        attn_scores = tf.expand_dims(attn_scores, axis=1)\n",
    "        \n",
    "        attn_scores = tf.matmul(attn_scores, enc_out, transpose_b=True)  # shape(b_sz, seq_len)\n",
    "        attn_weights = tf.nn.softmax(attn_scores, axis=-1)  # shape(b_sz, seq_len)\n",
    "        context_vector = tf.matmul(attn_weights, enc_out)  # shape(b_sz, hidden_sz) \n",
    "\n",
    "        context_vector = tf.squeeze(context_vector, axis=1)\n",
    "\n",
    "        dec_out_with_ctx = tf.concat(\n",
    "            [h, context_vector], \n",
    "            axis=-1\n",
    "        )  # shape(b_sz, 2 * hidden_sz)\n",
    "\n",
    "        logits = self.dense(dec_out_with_ctx)  \n",
    "        out = tf.argmax(logits, axis=-1, output_type=tf.int32)\n",
    "        return out, state\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loss函数以及训练逻辑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T06:28:07.287149900Z",
     "start_time": "2024-12-21T06:28:07.273385600Z"
    }
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    losses = tf.reduce_mean(losses)\n",
    "    return losses\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, enc_x, dec_x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(enc_x, dec_x)\n",
    "        loss = compute_loss(logits, y)\n",
    "\n",
    "    # compute gradient\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss\n",
    "\n",
    "def train(model, optimizer, seqlen):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for step in range(2000):\n",
    "        batched_examples, enc_x, dec_x, y = get_batch(32, seqlen)\n",
    "        loss = train_one_step(model, optimizer, enc_x, dec_x, y)\n",
    "        if step % 500 == 0:\n",
    "            print('step', step, ': loss', loss.numpy())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练迭代"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T06:28:22.566291200Z",
     "start_time": "2024-12-21T06:28:07.276997300Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 : loss 3.3021026\n",
      "step 500 : loss 1.7321354\n",
      "step 1000 : loss 0.16827048\n",
      "step 1500 : loss 0.06470987\n"
     ]
    },
    {
     "data": {
      "text/plain": "<tf.Tensor: shape=(), dtype=float32, numpy=0.022015508>"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(0.0005)\n",
    "model = mySeq2SeqModel()\n",
    "train(model, optimizer, seqlen=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 测试模型逆置能力\n",
    "首先要先对输入的一个字符串进行encode，然后在用decoder解码出逆置的字符串\n",
    "\n",
    "测试阶段跟训练阶段的区别在于，在训练的时候decoder的输入是给定的，而在预测的时候我们需要一步步生成下一步的decoder的输入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-12-21T06:28:22.727476100Z",
     "start_time": "2024-12-21T06:28:22.564197100Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, True, False, True, True, True, True, True, True]\n",
      "[('THFNYJRXKOMANSMUKNWI', 'IWNKUMSNAMOKXRJYNFHT'), ('HZEDLLPEZTFYLCGSTZZN', 'NZZTSGCLYFTZEPLLDEZH'), ('LQJZQQPHOWKGVKZADETJ', 'JTEDAZKVGKWOHPQQZJQL'), ('YCJKZLNZSDQWHMSIVSZA', 'AZSVISMHWQDSZNLZKJCY'), ('JNVMYCCXPXDZDRKFMPGY', 'YGPMFKRDZDXPXCCYMVNJ'), ('RRELKGQPIMJTJSTFZIBV', 'VBIZFTSJTJMIPQGKLERR'), ('DDNIOOQUTCLRJFYALREF', 'FERLAYFJRLCTUQOOINDD'), ('EQWUWEMVAWSFSKPTMTAM', 'MATMTPKSFSWAVMEWUWQE'), ('DPHFQCXUJMOHITWEYCBF', 'FBCYEWTIHOMJUXCQFHPD'), ('MGULJDXIAGTAIEPXUAMJ', 'JMAUXPEIATGAIXDJLUGM'), ('UVZBXOACAMXCOYVASVIF', 'FIVSAVYOCXMACAOXBZVU'), ('YKNOWTMNOUVSRCYSEYBT', 'TBYESYCRSVUONMTWONKY'), ('POKTWWNRCXIFAUGFOCBA', 'ABCOFGUAFIXCRNWWTKOP'), ('DYFYNQFRVDCDITRPGBOW', 'WOBGPRTIDCDVRFQNYFYD'), ('OUCMFVSQRZLSISIEIXES', 'SEXIEISISLZRQSVFMCUO'), ('EAXJIEXLRQSSDWXRLJQY', 'YQJLRXWDSSQRLXEIJXAE'), ('PXVZGXGFHNHWNGKTLAMU', 'UMALTKGNWHNHFGXGZVXP'), ('RJLQQSBACCPGPSISKTVV', 'VVTKSISPGPCCABSQQLJR'), ('XVMSMRCMNSPQVQZMNBCW', 'WCBNMZQVQPSNMCRMSMVX'), ('LTQWKHWLCTLHDBOHBLAA', 'AALBHOBDHLTCLWHKWQTL'), ('EHQORIYCSAPPBWYAMDVT', 'TVDMAYWBPPASCYIROQHE'), ('WKXGJNBJFLZJHASWUVWP', 'PWVUWSAHJZLFJBNJGXKW'), ('KTUCEKMRFYELRPWEVRMH', 'HMRVEWPRLEYFRMKECUTK'), ('VKXFBEZTCVVYLAWWBKQK', 'KQKBWWALYVVCTZEBFXKV'), ('YFGJCRBZNLRLPKTXQHTQ', 'QTHQXTKPLRLNZBRCJGFY'), ('XUJBPFWWLLWVOFULGJWU', 'UWJGLUFOVWLLWWFPBJUX'), ('SGABADDNYOLQKVZTEWUF', 'FUWETZVKQLOYNDDABAGS'), ('JNNIPJEEQUONNLLUIBIK', 'KIBIULLNNOUQEEJPINNJ'), ('GOHWTBAPHNUCXEZEFGJV', 'VJGFEZEXCUNHPABTWHOG'), ('VHYVWEHHTJMOHTMRPXAI', 'IAXPRMTHOMJTHHEWVYHV'), ('STVRILVJIQXINDUFBSGO', 'OGSBFUDNIXQIJVLIRVTS'), ('KFBZEPEBLSZRJWFVCFXR', 'RXFCVFWJRZSLBEPEZBFK')]\n"
     ]
    }
   ],
   "source": [
    "def sequence_reversal():\n",
    "    def decode(init_state, steps, enc_out):\n",
    "        b_sz = tf.shape(init_state[0])[0]\n",
    "        cur_token = tf.zeros(shape=[b_sz], dtype=tf.int32)\n",
    "        state = init_state\n",
    "        collect = []\n",
    "        for i in range(steps):\n",
    "            cur_token, state = model.get_next_token(cur_token, state, enc_out)\n",
    "            collect.append(tf.expand_dims(cur_token, axis=-1))\n",
    "        out = tf.concat(collect, axis=-1).numpy()\n",
    "        out = [''.join([chr(idx+ord('A')-1) for idx in exp]) for exp in out]\n",
    "        return out\n",
    "    \n",
    "    batched_examples, enc_x, _, _ = get_batch(32, 20)\n",
    "    enc_out, state = model.encode(enc_x)\n",
    "    return decode(state, enc_x.get_shape()[-1], enc_out), batched_examples\n",
    "\n",
    "def is_reverse(seq, rev_seq):\n",
    "    rev_seq_rev = ''.join([i for i in reversed(list(rev_seq))])\n",
    "    if seq == rev_seq_rev:\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "print([is_reverse(*item) for item in list(zip(*sequence_reversal()))])\n",
    "print(list(zip(*sequence_reversal())))"
   ]
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-12-21T06:28:22.728478900Z",
     "start_time": "2024-12-21T06:28:22.724943500Z"
    }
   },
   "execution_count": 6
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
