{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 生成数据集， 看明白即可无需填写代码\n",
    "#### '<font color=\"blue\">+</font>' 从高斯分布采样 (X, Y) ~ N(3, 6, 1, 1, 0).<br>\n",
    "#### '<font color=\"green\">o</font>' 从高斯分布采样  (X, Y) ~ N(6, 3, 1, 1, 0)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-07-12T09:38:22.653350Z",
     "iopub.status.busy": "2022-07-12T09:38:22.652655Z",
     "iopub.status.idle": "2022-07-12T09:38:22.658605Z",
     "shell.execute_reply": "2022-07-12T09:38:22.657762Z",
     "shell.execute_reply.started": "2022-07-12T09:38:22.653309Z"
    },
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from matplotlib import animation, rc\n",
    "from IPython.display import HTML\n",
    "import matplotlib.cm as cm\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "\n",
    "dot_num = 100\n",
    "x_p = np.random.normal(3., 1, dot_num)\n",
    "y_p = np.random.normal(6., 1, dot_num)\n",
    "y = np.ones(dot_num)\n",
    "C1 = np.array([x_p, y_p, y]).T\n",
    "\n",
    "x_n = np.random.normal(6., 1, dot_num)\n",
    "y_n = np.random.normal(3., 1, dot_num)\n",
    "y = np.zeros(dot_num)\n",
    "C2 = np.array([x_n, y_n, y]).T\n",
    "\n",
    "plt.scatter(C1[:, 0], C1[:, 1], c='b', marker='+')\n",
    "plt.scatter(C2[:, 0], C2[:, 1], c='g', marker='o')\n",
    "\n",
    "data_set = np.concatenate((C1, C2), axis=0)\n",
    "np.random.shuffle(data_set)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 建立模型\n",
    "建立模型类，定义loss函数，定义一步梯度下降过程函数\n",
    "\n",
    "填空一：实现sigmoid的交叉熵损失函数(不使用tf内置的loss 函数)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "epsilon = 1e-12\n",
    "class LogisticRegression():\n",
    "    def __init__(self):\n",
    "        self.W = tf.Variable(shape=[2, 1], dtype=tf.float32, \n",
    "            initial_value=tf.random.uniform(shape=[2, 1], minval=-0.1, maxval=0.1))\n",
    "        self.b = tf.Variable(shape=[1], dtype=tf.float32, initial_value=tf.zeros(shape=[1]))\n",
    "        \n",
    "        self.trainable_variables = [self.W, self.b]\n",
    "    @tf.function\n",
    "    def __call__(self, inp):\n",
    "        logits = tf.matmul(inp, self.W) + self.b # shape(N, 1)\n",
    "        pred = tf.nn.sigmoid(logits)\n",
    "        return pred\n",
    "\n",
    "@tf.function\n",
    "def compute_loss(pred, label):\n",
    "    if not isinstance(label, tf.Tensor):\n",
    "        label = tf.constant(label, dtype=tf.float32)\n",
    "    pred = tf.squeeze(pred, axis=1)\n",
    "    '''============================='''\n",
    "    #输入label shape(N,), pred shape(N,)\n",
    "    #输出 losses shape(N,) 每一个样本一个loss\n",
    "    #todo 填空一，实现sigmoid的交叉熵损失函数(不使用tf内置的loss 函数)\n",
    "    pred_clipped = tf.clip_by_value(pred, 1e-12, 1-1e-12)\n",
    "    losses = -(label * tf.math.log(pred_clipped) + (1-label) * tf.math.log(1-pred_clipped))\n",
    "    '''============================='''\n",
    "    loss = tf.reduce_mean(losses)\n",
    "    pred = tf.where(pred>0.5, tf.ones_like(pred), tf.zeros_like(pred))\n",
    "    accuracy = tf.reduce_mean(tf.cast(tf.equal(label, pred), dtype=tf.float32))\n",
    "    return loss, accuracy\n",
    "\n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred = model(x)\n",
    "        loss, accuracy = compute_loss(pred, y)\n",
    "\n",
    "    grads = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "    return loss, accuracy, model.W, model.b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 实例化一个模型，进行训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    model = LogisticRegression()\n",
    "    opt = tf.keras.optimizers.SGD(learning_rate=0.01)\n",
    "    x1, x2, y = list(zip(*data_set))\n",
    "    x = list(zip(x1, x2))\n",
    "    x = tf.constant(x, dtype=tf.float32)\n",
    "    y = tf.constant(y, dtype=tf.float32)\n",
    "    animation_fram = []\n",
    "    import sys\n",
    "    for i in range(200):\n",
    "        loss, accuracy, W_opt, b_opt = train_one_step(model, opt, x, y)\n",
    "        animation_fram.append(\n",
    "            (W_opt.numpy()[0, 0], W_opt.numpy()[1, 0], b_opt.numpy(), loss.numpy())\n",
    "        )\n",
    "        sys.stdout.write(\n",
    "            f'\\repoch: {i+1}\\t Loss: {loss.numpy():.4f}\\t Accuracy: {accuracy.numpy():.4f}'\n",
    "        )\n",
    "        sys.stdout.flush()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 结果展示，无需填写代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f, ax = plt.subplots(figsize=(6,4))\n",
    "f.suptitle('Logistic Regression Example', fontsize=15)\n",
    "plt.ylabel('Y')\n",
    "plt.xlabel('X')\n",
    "ax.set_xlim(0, 10)\n",
    "ax.set_ylim(0, 10)\n",
    "\n",
    "line_d, = ax.plot([], [], label='fit_line')\n",
    "C1_dots, = ax.plot([], [], '+', c='b', label='actual_dots')\n",
    "C2_dots, = ax.plot([], [], 'o', c='g' ,label='actual_dots')\n",
    "\n",
    "\n",
    "frame_text = ax.text(0.02, 0.95,'',horizontalalignment='left',verticalalignment='top', transform=ax.transAxes)\n",
    "# ax.legend()\n",
    "\n",
    "def init():\n",
    "    line_d.set_data([],[])\n",
    "    C1_dots.set_data([],[])\n",
    "    C2_dots.set_data([],[])\n",
    "    return (line_d,) + (C1_dots,) + (C2_dots,)\n",
    "\n",
    "def animate(i):\n",
    "    xx = np.arange(10, step=0.1)\n",
    "    a = animation_fram[i][0]\n",
    "    b = animation_fram[i][1]\n",
    "    c = animation_fram[i][2]\n",
    "    yy = a/-b * xx +c/-b\n",
    "    line_d.set_data(xx, yy)\n",
    "        \n",
    "    C1_dots.set_data(C1[:, 0], C1[:, 1])\n",
    "    C2_dots.set_data(C2[:, 0], C2[:, 1])\n",
    "    \n",
    "    frame_text.set_text('Timestep = %.1d/%.1d\\nLoss = %.3f' % (i+1, len(animation_fram), animation_fram[i][3]))\n",
    "    \n",
    "    return (line_d,) + (C1_dots,) + (C2_dots,)\n",
    "\n",
    "anim = animation.FuncAnimation(\n",
    "    f, animate, init_func=init,\n",
    "    frames=len(animation_fram), interval=30, blit=True\n",
    ")\n",
    "\n",
    "# HTML(anim.to_html5_video())\n",
    "HTML(anim.to_jshtml())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
