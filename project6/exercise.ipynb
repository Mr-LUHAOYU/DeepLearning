{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 诗歌生成"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 数据处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:54.927854300Z",
     "start_time": "2024-11-18T06:42:54.910914700Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 12:46:41.745390: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import collections\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import layers, optimizers, datasets\n",
    "\n",
    "start_token = 'bos'\n",
    "end_token = 'eos'\n",
    "\n",
    "def process_dataset(fileName):\n",
    "    examples = []\n",
    "    with open(fileName, 'r', encoding='utf-8') as fd:\n",
    "        for line in fd:\n",
    "            outs = line.strip().split(':')\n",
    "            content = ''.join(outs[1:])\n",
    "            ins = [start_token] + list(content) + [end_token] \n",
    "            if len(ins) > 200:\n",
    "                continue\n",
    "            examples.append(ins)\n",
    "            \n",
    "    counter = collections.Counter()\n",
    "    for e in examples:\n",
    "        for w in e:\n",
    "            counter[w]+=1\n",
    "    \n",
    "    sorted_counter = sorted(counter.items(), key=lambda x: -x[1])  # 排序\n",
    "    words, _ = zip(*sorted_counter)\n",
    "    words = ('PAD', 'UNK') + words[:len(words)]\n",
    "    word2id = dict(zip(words, range(len(words))))\n",
    "    id2word = {word2id[k]:k for k in word2id}\n",
    "    \n",
    "    indexed_examples = [[word2id[w] for w in poem]\n",
    "                        for poem in examples]\n",
    "    seqlen = [len(e) for e in indexed_examples]\n",
    "    \n",
    "    instances = list(zip(indexed_examples, seqlen))\n",
    "    \n",
    "    return instances, word2id, id2word\n",
    "\n",
    "def poem_dataset():\n",
    "    instances, word2id, id2word = process_dataset('./poems.txt')\n",
    "    ds = tf.data.Dataset.from_generator(\n",
    "        lambda: [ins for ins in instances], \n",
    "        (tf.int64, tf.int64), (tf.TensorShape([None]),\n",
    "        tf.TensorShape([]))\n",
    "    )\n",
    "    ds = ds.shuffle(buffer_size=10240)\n",
    "    ds = ds.padded_batch(100, padded_shapes=(tf.TensorShape([None]),tf.TensorShape([])))\n",
    "    ds = ds.map(lambda x, seqlen: (x[:, :-1], x[:, 1:], seqlen-1))\n",
    "    return ds, word2id, id2word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 模型代码， 完成建模代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:54.956383300Z",
     "start_time": "2024-11-18T06:42:54.936309600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class myRNNModel(keras.Model):\n",
    "    def __init__(self, w2id):\n",
    "        super(myRNNModel, self).__init__()\n",
    "        self.v_sz = len(w2id)\n",
    "        self.embed_layer = tf.keras.layers.Embedding(\n",
    "            self.v_sz, 64, batch_input_shape=[None, None]\n",
    "        )\n",
    "        self.rnncell = tf.keras.layers.SimpleRNNCell(128)\n",
    "        self.rnn_layer = tf.keras.layers.RNN(self.rnncell, return_sequences=True)\n",
    "        self.dense = tf.keras.layers.Dense(self.v_sz)\n",
    "    \n",
    "    @tf.function\n",
    "    def call(self, inp_ids):\n",
    "        # print(self.v_sz, inp_ids.shape)\n",
    "        # 嵌入层\n",
    "        inp_emb = self.embed_layer(inp_ids)  # (batch_size, timesteps, embed_dim)\n",
    "        # RNN 层\n",
    "        rnn_output = self.rnn_layer(inp_emb)  # (batch_size, timesteps, rnn_units)\n",
    "        # 全连接层\n",
    "        logits = self.dense(rnn_output)  # (batch_size, timesteps, vocab_size)\n",
    "        # print(type(logits))\n",
    "        return logits\n",
    "    \n",
    "    @tf.function\n",
    "    def get_next_token(self, x, state):\n",
    "        '''\n",
    "        shape(x) = [b_sz,] \n",
    "        '''\n",
    "    \n",
    "        inp_emb = self.embed_layer(x) #shape(b_sz, emb_sz)\n",
    "        h, state = self.rnncell.call(inp_emb, state) # shape(b_sz, h_sz)\n",
    "        logits = self.dense(h) # shape(b_sz, v_sz)\n",
    "        out = tf.argmax(logits, axis=-1)\n",
    "        return out, state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 一个计算sequence loss的辅助函数，只需了解用途。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:54.957390800Z",
     "start_time": "2024-11-18T06:42:54.945058400Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def mkMask(input_tensor, maxLen):\n",
    "    shape_of_input = tf.shape(input_tensor)\n",
    "    shape_of_output = tf.concat(axis=0, values=[shape_of_input, [maxLen]])\n",
    "\n",
    "    oneDtensor = tf.reshape(input_tensor, shape=(-1,))\n",
    "    flat_mask = tf.sequence_mask(oneDtensor, maxlen=maxLen)\n",
    "    return tf.reshape(flat_mask, shape_of_output)\n",
    "\n",
    "\n",
    "def reduce_avg(reduce_target, lengths, dim):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        reduce_target : shape(d_0, d_1,..,d_dim, .., d_k)\n",
    "        lengths : shape(d0, .., d_(dim-1))\n",
    "        dim : which dimension to average, should be a python number\n",
    "    \"\"\"\n",
    "    shape_of_lengths = lengths.get_shape()\n",
    "    shape_of_target = reduce_target.get_shape()\n",
    "    if len(shape_of_lengths) != dim:\n",
    "        raise ValueError(('Second input tensor should be rank %d, ' +\n",
    "                         'while it got rank %d') % (dim, len(shape_of_lengths)))\n",
    "    if len(shape_of_target) < dim+1 :\n",
    "        raise ValueError(('First input tensor should be at least rank %d, ' +\n",
    "                         'while it got rank %d') % (dim+1, len(shape_of_target)))\n",
    "\n",
    "    rank_diff = len(shape_of_target) - len(shape_of_lengths) - 1\n",
    "    mxlen = tf.shape(reduce_target)[dim]\n",
    "    mask = mkMask(lengths, mxlen)\n",
    "    if rank_diff!=0:\n",
    "        len_shape = tf.concat(axis=0, values=[tf.shape(lengths), [1]*rank_diff])\n",
    "        mask_shape = tf.concat(axis=0, values=[tf.shape(mask), [1]*rank_diff])\n",
    "    else:\n",
    "        len_shape = tf.shape(lengths)\n",
    "        mask_shape = tf.shape(mask)\n",
    "    lengths_reshape = tf.reshape(lengths, shape=len_shape)\n",
    "    mask = tf.reshape(mask, shape=mask_shape)\n",
    "\n",
    "    mask_target = reduce_target * tf.cast(mask, dtype=reduce_target.dtype)\n",
    "\n",
    "    red_sum = tf.reduce_sum(mask_target, axis=[dim], keepdims=False)\n",
    "    red_avg = red_sum / (tf.cast(lengths_reshape, dtype=tf.float32) + 1e-30)\n",
    "    return red_avg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 定义loss函数，定义训练函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:54.970012400Z",
     "start_time": "2024-11-18T06:42:54.955383400Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def compute_loss(logits, labels, seqlen):\n",
    "    losses = tf.nn.sparse_softmax_cross_entropy_with_logits(\n",
    "            logits=logits, labels=labels)\n",
    "    losses = reduce_avg(losses, seqlen, dim=1)\n",
    "    return tf.reduce_mean(losses)\n",
    "    \n",
    "@tf.function\n",
    "def train_one_step(model, optimizer, x, y, seqlen):\n",
    "    '''\n",
    "    完成一步优化过程，可以参考之前做过的模型 \n",
    "    '''\n",
    "    with tf.GradientTape() as tape:\n",
    "        # 前向传播\n",
    "        logits = model(x)  # shape: (batch_size, sequence_length, vocab_size)\n",
    "        \n",
    "        # 计算损失\n",
    "        loss = compute_loss(logits, y, seqlen)\n",
    "    \n",
    "    # 计算梯度\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # 更新权重\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss\n",
    "\n",
    "def train(epoch, model, optimizer, ds):\n",
    "    loss = 0.0\n",
    "    accuracy = 0.0\n",
    "    for step, (x, y, seqlen) in enumerate(ds):\n",
    "        loss = train_one_step(model, optimizer, x, y, seqlen)\n",
    "\n",
    "        if step % 50 == 0:\n",
    "            print('epoch', epoch, ': loss', loss.numpy())\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 训练优化过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:59.976692400Z",
     "start_time": "2024-11-18T06:42:54.960393200Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-27 12:46:44.888645: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1\n",
      "2024-11-27 12:46:44.934655: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:88:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2024-11-27 12:46:44.935014: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:89:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2024-11-27 12:46:44.935041: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-11-27 12:46:44.941113: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-11-27 12:46:44.941180: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-11-27 12:46:44.943656: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcufft.so.10\n",
      "2024-11-27 12:46:44.943929: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcurand.so.10\n",
      "2024-11-27 12:46:44.944390: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusolver.so.11\n",
      "2024-11-27 12:46:44.945224: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcusparse.so.11\n",
      "2024-11-27 12:46:44.945365: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudnn.so.8\n",
      "2024-11-27 12:46:44.946704: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2024-11-27 12:46:44.947421: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-11-27 12:46:45.110257: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 0 with properties: \n",
      "pciBusID: 0000:88:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2024-11-27 12:46:45.110611: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1733] Found device 1 with properties: \n",
      "pciBusID: 0000:89:00.0 name: NVIDIA GeForce RTX 3080 computeCapability: 8.6\n",
      "coreClock: 1.71GHz coreCount: 68 deviceMemorySize: 9.77GiB deviceMemoryBandwidth: 707.88GiB/s\n",
      "2024-11-27 12:46:45.111855: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1871] Adding visible gpu devices: 0, 1\n",
      "2024-11-27 12:46:45.111914: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\n",
      "2024-11-27 12:46:46.070572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1258] Device interconnect StreamExecutor with strength 1 edge matrix:\n",
      "2024-11-27 12:46:46.070624: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1264]      0 1 \n",
      "2024-11-27 12:46:46.070633: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 0:   N N \n",
      "2024-11-27 12:46:46.070638: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1277] 1:   N N \n",
      "2024-11-27 12:46:46.072573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 8102 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:88:00.0, compute capability: 8.6)\n",
      "2024-11-27 12:46:46.073265: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1418] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:1 with 8102 MB memory) -> physical GPU (device: 1, name: NVIDIA GeForce RTX 3080, pci bus id: 0000:89:00.0, compute capability: 8.6)\n",
      "2024-11-27 12:46:46.239420: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2024-11-27 12:46:46.241714: I tensorflow/core/platform/profile_utils/cpu_utils.cc:114] CPU Frequency: 2400000000 Hz\n",
      "2024-11-27 12:46:48.232756: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublas.so.11\n",
      "2024-11-27 12:46:49.205041: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcublasLt.so.11\n",
      "2024-11-27 12:46:49.205153: I tensorflow/stream_executor/cuda/cuda_blas.cc:1838] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 0 : loss 8.819988\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_one_step at 0x7fe6b3524310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "WARNING:tensorflow:5 out of the last 11 calls to <function train_one_step at 0x7fe6b3524310> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "epoch 0 : loss 6.7392507\n",
      "epoch 0 : loss 6.5729027\n",
      "epoch 0 : loss 6.5284057\n",
      "epoch 0 : loss 6.520408\n",
      "epoch 0 : loss 6.5873785\n",
      "epoch 0 : loss 6.492665\n",
      "epoch 0 : loss 6.574579\n",
      "epoch 0 : loss 6.530363\n",
      "epoch 1 : loss 6.6035676\n",
      "epoch 1 : loss 6.4748363\n",
      "epoch 1 : loss 6.4844017\n",
      "epoch 1 : loss 6.412931\n",
      "epoch 1 : loss 6.4022956\n",
      "epoch 1 : loss 6.336051\n",
      "epoch 1 : loss 6.2545834\n",
      "epoch 1 : loss 6.1453314\n",
      "epoch 1 : loss 6.122844\n",
      "epoch 2 : loss 6.254203\n",
      "epoch 2 : loss 5.9119473\n",
      "epoch 2 : loss 5.901873\n",
      "epoch 2 : loss 5.8921022\n",
      "epoch 2 : loss 5.8151\n",
      "epoch 2 : loss 5.921626\n",
      "epoch 2 : loss 5.9104853\n",
      "epoch 2 : loss 5.903154\n",
      "epoch 2 : loss 5.868508\n",
      "epoch 3 : loss 5.9410768\n",
      "epoch 3 : loss 5.7757573\n",
      "epoch 3 : loss 5.738744\n",
      "epoch 3 : loss 5.7591186\n",
      "epoch 3 : loss 5.749307\n",
      "epoch 3 : loss 5.740973\n",
      "epoch 3 : loss 5.735068\n",
      "epoch 3 : loss 5.7854915\n",
      "epoch 3 : loss 5.738645\n",
      "epoch 4 : loss 5.778041\n",
      "epoch 4 : loss 5.695814\n",
      "epoch 4 : loss 5.6809554\n",
      "epoch 4 : loss 5.6814985\n",
      "epoch 4 : loss 5.682462\n",
      "epoch 4 : loss 5.619029\n",
      "epoch 4 : loss 5.634596\n",
      "epoch 4 : loss 5.6240754\n",
      "epoch 4 : loss 5.582596\n",
      "epoch 5 : loss 5.682552\n",
      "epoch 5 : loss 5.3875093\n",
      "epoch 5 : loss 5.55506\n",
      "epoch 5 : loss 5.454062\n",
      "epoch 5 : loss 5.4282947\n",
      "epoch 5 : loss 5.4958296\n",
      "epoch 5 : loss 5.476015\n",
      "epoch 5 : loss 5.4289002\n",
      "epoch 5 : loss 5.596663\n",
      "epoch 6 : loss 5.5334606\n",
      "epoch 6 : loss 5.3962536\n",
      "epoch 6 : loss 5.3978953\n",
      "epoch 6 : loss 5.2732344\n",
      "epoch 6 : loss 5.405629\n",
      "epoch 6 : loss 5.2565107\n",
      "epoch 6 : loss 5.3478823\n",
      "epoch 6 : loss 5.260513\n",
      "epoch 6 : loss 5.4065704\n",
      "epoch 7 : loss 5.3811793\n",
      "epoch 7 : loss 5.2290263\n",
      "epoch 7 : loss 5.2812867\n",
      "epoch 7 : loss 5.260031\n",
      "epoch 7 : loss 5.200606\n",
      "epoch 7 : loss 5.310006\n",
      "epoch 7 : loss 5.291646\n",
      "epoch 7 : loss 5.251026\n",
      "epoch 7 : loss 5.244775\n",
      "epoch 8 : loss 5.3346734\n",
      "epoch 8 : loss 5.2109895\n",
      "epoch 8 : loss 5.258866\n",
      "epoch 8 : loss 5.149081\n",
      "epoch 8 : loss 5.235936\n",
      "epoch 8 : loss 5.1618004\n",
      "epoch 8 : loss 5.110249\n",
      "epoch 8 : loss 5.278732\n",
      "epoch 8 : loss 5.225802\n",
      "epoch 9 : loss 5.224814\n",
      "epoch 9 : loss 5.228272\n",
      "epoch 9 : loss 5.2487726\n",
      "epoch 9 : loss 5.096191\n",
      "epoch 9 : loss 5.108836\n",
      "epoch 9 : loss 5.1264367\n",
      "epoch 9 : loss 5.188364\n",
      "epoch 9 : loss 5.250699\n",
      "epoch 9 : loss 5.1859584\n"
     ]
    }
   ],
   "source": [
    "optimizer = optimizers.Adam(0.0005)\n",
    "train_ds, word2id, id2word = poem_dataset()\n",
    "model = myRNNModel(word2id)\n",
    "\n",
    "for epoch in range(10):\n",
    "    loss = train(epoch, model, optimizer, train_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# 生成过程"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-18T06:42:59.985060300Z",
     "start_time": "2024-11-18T06:42:59.980172600Z"
    },
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "水上春风起，风风落月中。eos来无处处，不见此人心。eos有无人事，何人不可知。eos来无处处，不见此人心。eos有\n"
     ]
    }
   ],
   "source": [
    "def gen_sentence():\n",
    "    state = [tf.random.normal(shape=(1, 128), stddev=0.5), tf.random.normal(shape=(1, 128), stddev=0.5)]\n",
    "    cur_token = tf.constant([word2id['bos']], dtype=tf.int32)\n",
    "    collect = [] # 保存每一步生成汉字对应于字典中的整数编码\n",
    "    '''\n",
    "    填空三 建立for循环，连续调用model.get_next_token函数，生成长度为50的诗句\n",
    "    '''\n",
    "    for _ in range(50):  # 生成50个token\n",
    "        next_token, state = model.get_next_token(cur_token, state)  # 获取下一个token，并更新状态\n",
    "        collect.append(next_token.numpy()[0])  # 将生成的token添加到collect列表中\n",
    "        cur_token = next_token  # 当前token为生成的下一个token\n",
    "\n",
    "    return [id2word[t] for t in collect]\n",
    "print(''.join(gen_sentence()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2024-11-18T06:42:59.981992500Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
